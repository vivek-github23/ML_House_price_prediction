{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fea3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/aakas/Desktop/Downloads/Chrome Downloads/output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40875ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preprocessed=pd.read_csv(\"C:/Users/aakas/Desktop/Downloads/Chrome Downloads/datatotest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8ba1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "15749/15749 [==============================] - 170s 11ms/step - loss: 1.1102 - accuracy: 0.5197 - val_loss: 1.0663 - val_accuracy: 0.5376\n",
      "Epoch 2/6\n",
      "15749/15749 [==============================] - 160s 10ms/step - loss: 1.0435 - accuracy: 0.5492 - val_loss: 1.0597 - val_accuracy: 0.5413\n",
      "Epoch 3/6\n",
      "15749/15749 [==============================] - 163s 10ms/step - loss: 0.9868 - accuracy: 0.5743 - val_loss: 1.0691 - val_accuracy: 0.5366\n",
      "Epoch 4/6\n",
      "15749/15749 [==============================] - 162s 10ms/step - loss: 0.9248 - accuracy: 0.6054 - val_loss: 1.0914 - val_accuracy: 0.5345\n",
      "Epoch 5/6\n",
      "15749/15749 [==============================] - 162s 10ms/step - loss: 0.8686 - accuracy: 0.6325 - val_loss: 1.1321 - val_accuracy: 0.5292\n",
      "Epoch 6/6\n",
      "15749/15749 [==============================] - 162s 10ms/step - loss: 0.8193 - accuracy: 0.6570 - val_loss: 1.1620 - val_accuracy: 0.5242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17fcb382390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# Drop rows with NaN values in 'review_text' column\n",
    "df_input = df[['review_text', 'n_votes', 'n_comments', 'rating']]\n",
    "df_input = df_input.dropna(subset=['review_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df_input[['review_text', 'n_votes', 'n_comments']], df_input['rating'],\n",
    "    test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['review_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['review_text'])\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "y_test = label_encoder.transform(test_labels)\n",
    "\n",
    "# Build a custom generator to handle sparse input\n",
    "class SparseSequence(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = (idx + 1) * self.batch_size\n",
    "        batch_X = self.X[start_idx:end_idx].toarray()  # Convert sparse matrix to NumPy array\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "        return batch_X, batch_y\n",
    "\n",
    "# Build the deep neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_tfidf.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the custom generator\n",
    "batch_size = 32\n",
    "train_generator = SparseSequence(X_train_tfidf, y_train, batch_size=batch_size)\n",
    "validation_generator = SparseSequence(X_test_tfidf, y_test, batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=6, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5870bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preprocessed['review_text'].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34959b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8438/8438 [==============================] - 21s 2ms/step\n",
      "[4 5 3 ... 3 3 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming your preprocessed test data has columns 'review_text', 'n_votes', 'n_comments'\n",
    "X_test_preprocessed_tfidf = tfidf_vectorizer.transform(test_data_preprocessed['review_text'])\n",
    "\n",
    "# Convert the SciPy sparse matrix to SparseTensor\n",
    "X_test_preprocessed_tfidf_coo = X_test_preprocessed_tfidf.tocoo()\n",
    "indices = np.column_stack((X_test_preprocessed_tfidf_coo.row, X_test_preprocessed_tfidf_coo.col))\n",
    "values = X_test_preprocessed_tfidf_coo.data\n",
    "dense_shape = X_test_preprocessed_tfidf_coo.shape\n",
    "\n",
    "X_test_preprocessed_tfidf_sparse = tf.SparseTensor(indices, values, dense_shape)\n",
    "\n",
    "# Reorder sparse matrix indices using tf.sparse.reorder\n",
    "X_test_preprocessed_tfidf_reordered = tf.sparse.reorder(X_test_preprocessed_tfidf_sparse)\n",
    "\n",
    "# Use the trained model to predict labels for the preprocessed test data\n",
    "y_pred = np.argmax(model.predict(X_test_preprocessed_tfidf_reordered), axis=-1)\n",
    "\n",
    "# Map the predicted labels back to their original classes using label_encoder\n",
    "predicted_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Print or save the predicted labels\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1d05e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46f8baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               review_id  Prediction\n",
      "0       583b1ad3d3ab9b0f967bc73c2f5d68d8           4\n",
      "1       e53e941fb1bd2f76fb270cedbb1479b8           5\n",
      "2       e2ab3d609db0a5c7aaed9ee136073c53           3\n",
      "3       740ebae0f5b7fd0b7c9890ef306099f6           2\n",
      "4       f4812f3464aaf50d56480025afb22c57           3\n",
      "...                                  ...         ...\n",
      "269995  1bccc443a9a97a3fd0b45646f39b6465           3\n",
      "269996  ea52067f73ba6de7563eaf4567d423ef           5\n",
      "269997  153121e003a1d19db89d45723dd0ad8e           3\n",
      "269998  ce89b8c499252125f8983ef04a0d4ad9           3\n",
      "269999  454e60cbf37ec1479d0e2438cab10108           5\n",
      "\n",
      "[270000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_test is your original DataFrame and 'review_id' is a column in it\n",
    "# If df_test is already a DataFrame, you might want to access 'review_id' directly.\n",
    "# If df_test is not a DataFrame, replace df_test with your actual DataFrame.\n",
    "\n",
    "df1 = test_data_preprocessed['review_id'].reset_index(drop=True)  # Reset index\n",
    "df2 = pd.Series(predicted_labels, name='Prediction')   # Convert NumPy array to Pandas Series\n",
    "\n",
    "# Merge the DataFrames using pd.concat\n",
    "merged_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_data3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa3c9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9a0f14b836e2634b89a6a7d4d9aa184</td>\n",
       "      <td>23308084</td>\n",
       "      <td>7bde8725cbb9ceb697c72c12262dff53</td>\n",
       "      <td>complaint not trilogyi want fantastic end rene...</td>\n",
       "      <td>Thu Jul 23 22:27:34 -0700 2015</td>\n",
       "      <td>Wed May 04 20:04:40 -0700 2016</td>\n",
       "      <td>Wed May 04 00:00:00 -0700 2016</td>\n",
       "      <td>Mon Apr 25 00:00:00 -0700 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3af7e1cda1d80d6a6d73e06eab301368</td>\n",
       "      <td>23310751</td>\n",
       "      <td>a9aa8356ef1ca470c98712e17005517b</td>\n",
       "      <td>read 2 days ago remember nothing disappoint</td>\n",
       "      <td>Thu Oct 13 19:33:58 -0700 2016</td>\n",
       "      <td>Thu Oct 13 19:34:28 -0700 2016</td>\n",
       "      <td>Mon Oct 10 00:00:00 -0700 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd669721e136c1be47d739b14fa23d20</td>\n",
       "      <td>210252</td>\n",
       "      <td>8739baeb543858142605442041d79524</td>\n",
       "      <td>not fan first monster blood second addition pl...</td>\n",
       "      <td>Thu Feb 27 01:44:54 -0800 2014</td>\n",
       "      <td>Sun Oct 18 20:46:53 -0700 2015</td>\n",
       "      <td>Thu Feb 27 00:00:00 -0800 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aec624fd1ad0034f2553b7dc55ee1cd0</td>\n",
       "      <td>472392</td>\n",
       "      <td>1791472bb94c9733802303ecf34d9c53</td>\n",
       "      <td>love theo much interaction brother absolutely ...</td>\n",
       "      <td>Sun Aug 28 08:19:18 -0700 2016</td>\n",
       "      <td>Sat Sep 03 17:42:00 -0700 2016</td>\n",
       "      <td>Fri Sep 02 03:12:59 -0700 2016</td>\n",
       "      <td>Sun Aug 28 00:00:00 -0700 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d67aef0988e1814a819259eb11c92788</td>\n",
       "      <td>17675462</td>\n",
       "      <td>d98212782db1271607a94c5836ef6189</td>\n",
       "      <td>good book get middle part look forward read se...</td>\n",
       "      <td>Mon Jan 11 07:20:47 -0800 2016</td>\n",
       "      <td>Mon Mar 14 07:30:27 -0700 2016</td>\n",
       "      <td>Sun Mar 13 00:00:00 -0800 2016</td>\n",
       "      <td>Mon Jan 11 00:00:00 -0800 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629995</th>\n",
       "      <td>34d07fb9e04bfbbb17371435223b120e</td>\n",
       "      <td>296182</td>\n",
       "      <td>3de0c7af9700e937b488e129ca3c9cc9</td>\n",
       "      <td>edit 112613 advice regard vorkosigan saga boil...</td>\n",
       "      <td>Sat Jul 06 23:27:10 -0700 2013</td>\n",
       "      <td>Thu Jul 10 23:35:49 -0700 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629996</th>\n",
       "      <td>c3d7ca2c133140d684ab4d958d5e4ee9</td>\n",
       "      <td>5776788</td>\n",
       "      <td>96a1196bfce7fc2d4d4e589d990badc9</td>\n",
       "      <td>not book would likely pick read get via nook f...</td>\n",
       "      <td>Tue Oct 19 17:38:21 -0700 2010</td>\n",
       "      <td>Wed Dec 22 09:46:33 -0800 2010</td>\n",
       "      <td>Tue Dec 21 00:00:00 -0800 2010</td>\n",
       "      <td>Sat Dec 18 00:00:00 -0800 2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629997</th>\n",
       "      <td>60982541be85a0611e9634b4f63d0cb0</td>\n",
       "      <td>3648</td>\n",
       "      <td>a857718ca7e70b8c0ffc5ead14512fb8</td>\n",
       "      <td>book summary boast spectacular story write fla...</td>\n",
       "      <td>Thu Jun 08 22:25:07 -0700 2017</td>\n",
       "      <td>Thu Jun 08 22:28:54 -0700 2017</td>\n",
       "      <td>Fri Jun 09 07:16:37 -0700 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629998</th>\n",
       "      <td>b43eaf1760e1b11bc224815a3f3c48a3</td>\n",
       "      <td>13507212</td>\n",
       "      <td>393045562fb081cf0a6975a2f6b91908</td>\n",
       "      <td>excellent sequel wolf hall cover shorter perio...</td>\n",
       "      <td>Sun Jan 08 08:38:38 -0800 2017</td>\n",
       "      <td>Sun Jan 15 06:16:15 -0800 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629999</th>\n",
       "      <td>2c2822bfcdeb65ca48db551d4cfd16ef</td>\n",
       "      <td>23857998</td>\n",
       "      <td>941af42d5d9c694bbbbf8b4f8297b8c7</td>\n",
       "      <td>beautifully complicate different story somethi...</td>\n",
       "      <td>Fri Apr 17 09:59:49 -0700 2015</td>\n",
       "      <td>Fri Apr 17 13:40:44 -0700 2015</td>\n",
       "      <td>Fri Apr 17 14:47:26 -0700 2015</td>\n",
       "      <td>Fri Apr 17 00:00:00 -0700 2015</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id   book_id  \\\n",
       "0       d9a0f14b836e2634b89a6a7d4d9aa184  23308084   \n",
       "1       3af7e1cda1d80d6a6d73e06eab301368  23310751   \n",
       "2       dd669721e136c1be47d739b14fa23d20    210252   \n",
       "3       aec624fd1ad0034f2553b7dc55ee1cd0    472392   \n",
       "4       d67aef0988e1814a819259eb11c92788  17675462   \n",
       "...                                  ...       ...   \n",
       "629995  34d07fb9e04bfbbb17371435223b120e    296182   \n",
       "629996  c3d7ca2c133140d684ab4d958d5e4ee9   5776788   \n",
       "629997  60982541be85a0611e9634b4f63d0cb0      3648   \n",
       "629998  b43eaf1760e1b11bc224815a3f3c48a3  13507212   \n",
       "629999  2c2822bfcdeb65ca48db551d4cfd16ef  23857998   \n",
       "\n",
       "                               review_id  \\\n",
       "0       7bde8725cbb9ceb697c72c12262dff53   \n",
       "1       a9aa8356ef1ca470c98712e17005517b   \n",
       "2       8739baeb543858142605442041d79524   \n",
       "3       1791472bb94c9733802303ecf34d9c53   \n",
       "4       d98212782db1271607a94c5836ef6189   \n",
       "...                                  ...   \n",
       "629995  3de0c7af9700e937b488e129ca3c9cc9   \n",
       "629996  96a1196bfce7fc2d4d4e589d990badc9   \n",
       "629997  a857718ca7e70b8c0ffc5ead14512fb8   \n",
       "629998  393045562fb081cf0a6975a2f6b91908   \n",
       "629999  941af42d5d9c694bbbbf8b4f8297b8c7   \n",
       "\n",
       "                                              review_text  \\\n",
       "0       complaint not trilogyi want fantastic end rene...   \n",
       "1             read 2 days ago remember nothing disappoint   \n",
       "2       not fan first monster blood second addition pl...   \n",
       "3       love theo much interaction brother absolutely ...   \n",
       "4       good book get middle part look forward read se...   \n",
       "...                                                   ...   \n",
       "629995  edit 112613 advice regard vorkosigan saga boil...   \n",
       "629996  not book would likely pick read get via nook f...   \n",
       "629997  book summary boast spectacular story write fla...   \n",
       "629998  excellent sequel wolf hall cover shorter perio...   \n",
       "629999  beautifully complicate different story somethi...   \n",
       "\n",
       "                            date_added                    date_updated  \\\n",
       "0       Thu Jul 23 22:27:34 -0700 2015  Wed May 04 20:04:40 -0700 2016   \n",
       "1       Thu Oct 13 19:33:58 -0700 2016  Thu Oct 13 19:34:28 -0700 2016   \n",
       "2       Thu Feb 27 01:44:54 -0800 2014  Sun Oct 18 20:46:53 -0700 2015   \n",
       "3       Sun Aug 28 08:19:18 -0700 2016  Sat Sep 03 17:42:00 -0700 2016   \n",
       "4       Mon Jan 11 07:20:47 -0800 2016  Mon Mar 14 07:30:27 -0700 2016   \n",
       "...                                ...                             ...   \n",
       "629995  Sat Jul 06 23:27:10 -0700 2013  Thu Jul 10 23:35:49 -0700 2014   \n",
       "629996  Tue Oct 19 17:38:21 -0700 2010  Wed Dec 22 09:46:33 -0800 2010   \n",
       "629997  Thu Jun 08 22:25:07 -0700 2017  Thu Jun 08 22:28:54 -0700 2017   \n",
       "629998  Sun Jan 08 08:38:38 -0800 2017  Sun Jan 15 06:16:15 -0800 2017   \n",
       "629999  Fri Apr 17 09:59:49 -0700 2015  Fri Apr 17 13:40:44 -0700 2015   \n",
       "\n",
       "                               read_at                      started_at  \\\n",
       "0       Wed May 04 00:00:00 -0700 2016  Mon Apr 25 00:00:00 -0700 2016   \n",
       "1       Mon Oct 10 00:00:00 -0700 2016                             NaN   \n",
       "2       Thu Feb 27 00:00:00 -0800 2014                             NaN   \n",
       "3       Fri Sep 02 03:12:59 -0700 2016  Sun Aug 28 00:00:00 -0700 2016   \n",
       "4       Sun Mar 13 00:00:00 -0800 2016  Mon Jan 11 00:00:00 -0800 2016   \n",
       "...                                ...                             ...   \n",
       "629995                             NaN                             NaN   \n",
       "629996  Tue Dec 21 00:00:00 -0800 2010  Sat Dec 18 00:00:00 -0800 2010   \n",
       "629997  Fri Jun 09 07:16:37 -0700 2017                             NaN   \n",
       "629998                             NaN                             NaN   \n",
       "629999  Fri Apr 17 14:47:26 -0700 2015  Fri Apr 17 00:00:00 -0700 2015   \n",
       "\n",
       "        n_votes  n_comments  rating  \n",
       "0             1           1       5  \n",
       "1             3           0       2  \n",
       "2             0           0       2  \n",
       "3             0           0       4  \n",
       "4             0           0       4  \n",
       "...         ...         ...     ...  \n",
       "629995        8           3       4  \n",
       "629996        0           0       3  \n",
       "629997       13           0       2  \n",
       "629998        0           0       5  \n",
       "629999        7           0       4  \n",
       "\n",
       "[630000 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d1897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
